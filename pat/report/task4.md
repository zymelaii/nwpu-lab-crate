# 任务说明

基于决策树构建机器学习模型，根据乘客的特征预测其在泰坦尼克号沉船事件中是否幸存。

# 关于数据集

本实验采用 [Titanic Dataset](https://www.kaggle.com/datasets/brendan45774/test-file)，该数据集中列举了泰坦尼克号乘客的特征与其是否生还的信息。每个乘客的特征包括客舱登记、性别、年龄、同行的同伴/配偶数量、船票编号、票价等。生还信息为 Survived，作为真实值标签。

# 方法简介

观察数据集，可发现 Age 等列均存在缺失，对于数值型列取均值填充，对于离散值的列直接舍弃。另可以观察到 Ticket 列中并非全部行都为纯数值型，且存在空缺，故提取值中的票价数值，并用提取后的均值填充空闲列。

结合现实状况，可以初步筛去幸存与否决策的低重要性特征，如姓名、住址等，可得到初步的输入特征 Pclass、Sex、Age、SibSp、Parch、Fare、Ticket；其中 Sex 为 female、male 离散字符串取值，无法直接应用于训练，故使用 One-Hot 编码将其转换为两个二值特征。

> 考虑船票号码 Ticket 的原因在于相近的 Ticket 值往往在空间位置上相近的，故而其能够一定程度上反映事件发生时各游客在船上的位置，而这一点则很可能影响到得到救援的速度，并进一步影响该乘客存活与否

综上可设计以下流程：

1. 完成数据清洗与转换，填充空缺数据
2. 使用 One-Hot 编码转换离散非数值特征
3. 抽取若干特征列作为训练集样本输入，以决策树最大深度作为搜索空间，进行 10 折交叉验证网格搜索
4. 在训练集上评估预测准确率与各特征的重要性

# 实验结果

以 Pclass、Sex、Age、SibSp、Parch、Fare、Ticket 为特征时，交叉验证分数 0.8159300873907614，准确率 0.8283，各特征重要性：

|  Age   | Fare  | Parch  | Pclass |  Sex   | SibSp  | Ticket |
| :----: | :---: | :----: | :----: | :----: | :----: | :----: |
| 0.0575 | 0505  | 0.0000 | 0.2125 | 0.6257 | 0.0447 | 0.0091 |

进一步剔除 Parch 特征得到交叉验证分数 0.8170911360799001，准确率 0.8743，各特征重要性：

|  Age   |  Fare  | Pclass |  Sex   | SibSp  | Ticket |
| :----: | :----: | :----: | :----: | :----: | :----: |
| 0.0785 | 0.0718 | 0.1677 | 0.4939 | 0.0536 | 0.1344 |

进一步剔除 SibSp 特征得到交叉验证分数 0.8148439450686642，准确率 0.9540，各特征重要性：

|  Age   |  Fare  | Pclass |  Sex   | Ticket |
| :----: | :----: | :----: | :----: | :----: |
| 0.1181 | 0.1751 | 0.1250 | 0.3476 | 0.2343 |

此时各特征对于决策的贡献已较为清晰，其中性别和船票号码占主导地位，年龄、票价、船票类别则呈现相近的贡献。

在此基础上分别剔除除 Sex 外的其余特征，观察结果：

|  Age   |  Fare  | Pclass | Ticket |  Sex   |       Score        | Accuracy |
| :----: | :----: | :----: | :----: | :----: | :----------------: | :------: |
|   /    | 0.2234 | 0.0992 | 0.2814 | 0.3960 | 0.8170786516853932 |  0.9136  |
| 0.0922 |   /    | 0.2031 | 0.1065 | 0.5982 | 0.8093133583021224 |  0.8260  |
| 0.0909 | 0.1258 |   /    | 0.2452 | 0.5381 | 0.8227091136079899 |  0.8552  |
| 0.1428 | 0.1862 | 0.1881 |   /    | 0.4828 | 0.8171660424469411 |  0.8777  |

根据以上结果，仅当剔除重要性最低的 Age 时，准确率的下降相对较小，而其余三个特征中的任意一个被剔除都会导致准确率的大幅度下降，而交叉验证分数并无明显变化。

故而可以判断由 Age、Fare、Pclass、Sex、Ticket 组成输入特征，训练出的深度为 10 的决策树取得最优的训练效果。

# 遇到的问题及解决方法

**问题**

训练集数据中数值特征 Age、非数值特征 Cabin 存在大量特征，同时 Ticket 存在部分为非数值型字符串，使用空缺值直接训练会导致模型出现偏差，而 Cabin、Ticket 两个特征无法直接用作训练。

**方案**

数据清洗，用年龄的均值填充缺失值，同时舍弃无法填充的离散 Cabin 特征；对于 Ticket 做预处理，提取特征值中的数值作为该特征的新值。

**问题**

Sex 特征作为直觉上重要的特征，其值 Male、Female 无法被决策树直接处理。

**方案**

使用 One-Hot 编码，将 Sex 特征编码到 Sex=Male、Sex=Female 两个 0-1 二值特征。
